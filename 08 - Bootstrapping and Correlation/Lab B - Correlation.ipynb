{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07b - Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# makes our plots interactive\n",
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "# choose colors that work for most color-blind folks\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import util.lab07utils as utils\n",
    "\n",
    "utils.formatDataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technical note: like Lab 06, this notebook uses the interactive features of JuPyter notebooks. This means that when you generate a plot, you can keep adding things to that plot, so long as you don't either 1) make a new figure or 2) click the blue \"power button\" in the top-right corner of the figure.\n",
    "\n",
    "To prevent yourself from accidentally plotting into a figure you didn't mean to plot into, make sure to include a `plt.figure()` in each cell where you're making a plot. You'll want to do this right before you do the plotting commands (`sns.distplot`, `plt.plot`,etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we discussed several views of what the correlation means.\n",
    "\n",
    "#### Q Which one is your favorite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, the correlation is represented by the letter $r$.\n",
    "\n",
    "Why $r$? The $r$ stands for *regression*, and we learned in class that there's a close connection between regression and correlation.\n",
    "\n",
    "But why *regression*? Doesn't that usually mean moving backwards? \n",
    "\n",
    "If you're curious, check out the material below for a bit of history, otherwise you can move on to the next section.\n",
    "\n",
    "In fact, it does mean moving backwards in statistics as well. The term goes back to the late 19th century, when statistics was being developed, primarily for the purpose of measuring human populations. Of particular interest to statisticians at this time was the science of heredity (biological evolution being one of the first non-deterministic scientific theories). \n",
    "\n",
    "Note also that at this time, most scientists and mathematicians were aristocratic or independently wealthy males, as only a person with a steady income from other sources could afford the luxury of experimental apparatus and uninterrupted study, and women were largely barred. The Western European upper classes of this time were enamored with the theories of scientific racism and sexism, which helped to assuage the cognitive dissonance caused by combining their liberal sentiments about all persons being created equal with the material fact that equal opportunity was denied to almost all persons in their society, and they kind of liked it that way.\n",
    "\n",
    "Enter [Sir Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton), the half-cousin of Charles Darwin and the scion of a Quaker family who made their fortune dealing arms [(I am not making this up)](https://theironroom.wordpress.com/2015/03/23/faith-and-disunity-samuel-galton-and-the-quakers/). He was concerned to know whether the male children of tall fathers were themselves taller than average -- prejudices regarding height being then more explicit then than [they are now](http://gladwell.com/blink/why-do-we-love-tall-men/). He collected the relevant data and then invented linear modeling to summarize it. \n",
    "\n",
    "Sir Francis was delighted to discover that the average height of the son was correlated with the average height of the father, but horrified to see that the slope of this line was less than 1! This meant that, on average, sons would be shorter than their fathers, and their sons shorter than them, and so on. He termed this phenomenon [*regression to mediocrity*](https://books.google.com/books?id=JPcRAAAAYAAJ&pg=PA246#v=onepage&q&f=false), and the term stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate some correlated data.\n",
    "\n",
    "The simplest way to generate data that is dependent is to first sample one set of random numbers, then generate another set whose values depend on the first. \n",
    "\n",
    "In this example, we create correlated data by first sampling an array of x values from a standard normal and then generating an array of ys by adding another set of random numbers to the xs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numPoints = 50\n",
    "noise_level = 2\n",
    "\n",
    "def generateData(numPoints,noise_level):\n",
    "    xs = np.random.standard_normal(size=numPoints)\n",
    "    ys = np.asarray([x+np.random.standard_normal()*noise_level\n",
    "                 for x in xs])\n",
    "    \n",
    "    return xs,ys\n",
    "\n",
    "xs,ys = generateData(numPoints,noise_level)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(xs,ys,\n",
    "            s=24,linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python offers a number of options for computing the correlation coefficient of two or more datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy offers the `corrcoef` function. This function returns a correlation matrix -- the `ij`th entries of this matrix tell you the correlation between the `i`th and `j`th arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What do you expect the correlation coefficient to be when `i == j` (i.e. for the diagonal elements of the matrix)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `corrcoef` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.corrcoef(xs,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the `noise_level` above.\n",
    "\n",
    "#### Q What happens to the  correlation coefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas offers a way to calculate the correlation matrix of the numeric columns in a data frame. Data frames have a method, `.corr` that returns the correlation matrix as a data frame.\n",
    "\n",
    "First, we have to get our data into the pandas `DataFrame` format. Do this in the cell below using `pd.DataFrame.from_dict` or `.from_items`, then calculate the correlation matrix using `data.corr()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn's prime directive is to make things that people do frequently as painless as possible. Since plotting pairs of things and calculating their correlation is a very common step in data analysis, seaborn makes it extremely easy.\n",
    "\n",
    "Use the function `sns.jointplot` to simultaneously make a scatter plot of the data, calculate the correlation coefficient, and get a bootstrapped $p$-value for the correlation (it's easier than it sounds!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed a formula for the correlation coefficient, $r$. It is reproduced below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "r = \\frac{1}{n-1} \\sum_{i=1}^{n}{\\frac{(x_i-\\mu_x)^2}{s_x}\n",
    "                                  \\frac{(y_i-\\mu_y)^2}{s_y}}\n",
    "$$\n",
    "\n",
    "where $s_a$ is the estimated population standard deviation of a variable $a$:\n",
    "\n",
    "$$\n",
    "s_a = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n}{\\left(a_i-\\mu_a\\right)^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this formula to define a function that will compute the correlation coefficient of any pair of random variables, then run that function on our data.\n",
    "\n",
    "Hint: the function `np.std` takes an argument `ddof` that is helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Confidence Intervals on Correlations with Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first half of the lab, we examined the bootstrapping procedure for estimating the sampling distribution of statistics. Now, we'll use that procedure to estimate the sampling distribution of the correlation coefficient.\n",
    "\n",
    "In the cell below, implement the bootstrap for the correlation coefficient. That is, generate a dataset of size $N$ (aka `numPoints`). Then, draw $N$ pairs of `x` and `y` values from the dataset (with replacement) and calculate the correlation coefficient. You can use `np.random.randint(0,len(data)` to generate a random list of indices for sampling with replacement. Draw pairs from the same dataset at least 1000 times and then plot the distribution of the resulting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given (an approximation to) the sampling distribution, we can calculate a confidence interval at confidence level $\\alpha$ by dropping the extreme values of the sampling distribution -- the lower bound of the confidence interval is the $\\alpha/2$th percentile of the sampled values and the upper bound of the confidence interval is the $1-\\alpha/2$th percentile.\n",
    "\n",
    "Compute the confidence interval for your correlation coefficient in the cell below. Use, as we do out of habit, $\\alpha=0.05$. Hint: look up `np.percentile`. Notice that it takes percentiles as values between 0 and 100, not between 0 and 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can compute a confidence interval, we can compute a p-value.\n",
    "\n",
    "#### Q Briefly, how would you check if a result is significant at a level $\\alpha$ using the set of sampled $r$s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cautionary Tales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to not implying causation, correlation is also frequently misinterpreted in the following way: uncorrelated variables have no dependence on each other. This is untrue, as indicated in the Kendrick Kay lecture and in class.\n",
    "\n",
    "#### Q What can we actually say about the (lack of a) relationship between two uncorrelated variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, let's play with some data that is poorly described by its correlation.\n",
    "\n",
    "The easiest way to generate this data is to generate random values for $x$ and then generate each $y$ by applying some function to each $x$ and adding some scaled normal noise to the output (here, using the variable `noise_level`).\n",
    "\n",
    "For example, if we wanted to generate data that is well-modeled by a parabola, we'd generate some random values for $x$, then square each of them and add normally-distributed random numbers to each of them.\n",
    "\n",
    "Here are some ideas for functions to try:\n",
    "\n",
    "- `np.power`\n",
    "- `np.sin`, `.cos`, `.tan`, or other trigonometric functions\n",
    "- `min`, `max`\n",
    "- a function that draws a random number, like `np.random.choice` or `np.random.randint`\n",
    "\n",
    "Do this for at least two different functions. Write some notes below and be ready to share what you found with the class. Compute the correlation coefficient and plot the data using the cell below. The line drawn on the figure is the linear model presumed by the correlation coefficient. Keep the number of points high (over 100) so that the sample-to-sample variation is small.\n",
    "\n",
    "**Optional challenge for folks looking to take their Python to the next level**\n",
    "\n",
    "It's fairly straightforward to implement the procedure described above using list comprehensions (stuff like `[f(x) for x in xs]`). You can make your code even cleaner and easier to use by defining `f` as a variable, like `f = np.square`, above the list comprehension. However, that only works with functions that take in just one argument. If you'd like to both make your code cleaner and expand the kinds of functions you can use, read up on [lambda functions](http://stackoverflow.com/questions/890128/why-are-python-lambdas-useful) and use them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numPoints = 500\n",
    "noise_level = 1\n",
    "x_spread = 3\n",
    "\n",
    "x = [] #your code here\n",
    "y = [] #your code here\n",
    "\n",
    "df = pd.DataFrame.from_dict({'x':x,'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "scatter_kws = {'alpha':0.15,\n",
    "                'color':'k',\n",
    "                 's':72}\n",
    "\n",
    "sns.regplot(data=df,x='x',y='y',\n",
    "            scatter_kws=scatter_kws);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:neur299]",
   "language": "python",
   "name": "conda-env-neur299-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
