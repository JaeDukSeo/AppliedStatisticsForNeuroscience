{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/HWNI_logo.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Linear Transformations - Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes our plots interactive\n",
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import util.utils as utils\n",
    "import util.shared as shared\n",
    "\n",
    "shared.format_plots()\n",
    "shared.format_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Linear Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every matrix defines a *linear transformation*. Transformation, here, is a synonym for \"function\", but emphasizes that we want to picture our function as something that changes the shape of the whole space of things it takes as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you'll find a tool for visualizing how linear transformations warp space in two dimensions. In this lab, we'll step through various kinds of linear transformations and try to develop an intuition for what they do.\n",
    "\n",
    "First, we'll define some transformation matrices. Once you've made it through the lab, you're encouraged to make your own matrices (a fun example: make random matrices using functions in `np.random` and see what they do) and to play around with the visualizer. As another example, here's a challenge: make a function that takes in a vector and return a matrix that, when multiplied by a new vector, computes the dot product of that new vector with the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eye = [[1,0],\n",
    "       [0,1]]\n",
    "\n",
    "scaling1 = [[3,0],\n",
    "            [0,3]]\n",
    "\n",
    "scaling2 = [[1/2,0],\n",
    "            [0,1/2]]\n",
    "\n",
    "shear = [[1,2],\n",
    "         [0,1]]\n",
    "\n",
    "simple_reflection = [[0,1],\n",
    "                     [1,0]]\n",
    "\n",
    "negative_reflection = [[0,-1],\n",
    "                       [-1,0]]\n",
    "\n",
    "\n",
    "rank0 = [[0,0],\n",
    "         [0,0]]\n",
    "\n",
    "rank1 = [[1,2],\n",
    "         [1,2]]\n",
    "\n",
    "rot90 = utils.make_rotation(theta=np.pi/2)\n",
    "\n",
    "rot180 = utils.make_rotation(theta=np.pi) #uncomment, your code here\n",
    "\n",
    "def make_inner_product_matrix(vector):\n",
    "    return [vector,[0,0]]\n",
    "\n",
    "innerp = make_inner_product_matrix([1,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll learn more about why each transformation has its particular name later on.\n",
    "\n",
    "The animation below will work by plotting a mesh, or grid, of points and then applying the transformation to them.\n",
    "\n",
    "The two meshes defined below are particularly useful, but you can define your own if you'd like. Type `utils.setup_plot?` to learn how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_square_mesh = {'delta':0.1,\n",
    "                    'x_min' :0,\n",
    "                    'x_max' : 1,\n",
    "                    'y_min' : 0,\n",
    "                    'y_max' : 1,}\n",
    "\n",
    "all_quadrants_mesh = {'delta':0.1,\n",
    "                    'x_min' :-0.5,\n",
    "                    'x_max' : 0.5,\n",
    "                    'y_min' : -0.5,\n",
    "                    'y_max' : 0.5,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate an animation, you need to pick a `mesh` and a `transform` in the cell below, then run the cell beneath it.\n",
    "\n",
    "Run these two cells as they are and check out the output.\n",
    "\n",
    "If the animation runs too fast, increase the value of the `delay` argument below. If it runs too slow, decrease the value of `delay` (minimum `0`). You can learn more about `animate_transformation` by typing `utils.animate_transformation?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = scaling2\n",
    "\n",
    "transform = rank0\n",
    "\n",
    "transform = np.dot(scaling1,rot90)\n",
    "transform = np.random.randn(2,2)\n",
    "\n",
    "transform = rot180\n",
    "#transform = negative_reflection\n",
    "\n",
    "transform = innerp\n",
    "\n",
    "mesh = unit_square_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter,figure,axes = utils.setup_plot(transform,\n",
    "                                mesh_properties=mesh,\n",
    "                                plot_columns=True,\n",
    "                                plot_eigenvectors=False)\n",
    "\n",
    "utils.animate_transformation(transform,scatter,figure,\n",
    "                                mesh_properties=mesh,\n",
    "                                 delay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll work through each class of matrices and learn a bit about them. You'll want to visualize each transformation with both the `unit_square` grid and the `all_quadrants` grid. If you think you've got the hang of one class, try making a new matrix of the same type (`your_matrix = [[a,b],[c,d]]`) and check to see if you're right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scaling` \n",
    "\n",
    "A scaling matrix just changes the lengths of vectors, all by the same amount.\n",
    "\n",
    "#### Q1 How can we tell a matrix is a scaling matrix just by looking at its entries? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> **It has zeros off the diagonal and the same number in each diagonal position. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis vectors determine the bottom and left sides of a square with area 1. We call this the \"unit\" square. After the basis vectors are transformed, they still map out a four-sided figure -- often a rectangle or a parallelogram.\n",
    "\n",
    "#### Q2 What four-sided shape do the basis vectors define when they're transformed by a scaling matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** It's still a square! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important property of a matrix is its *determinant*.\n",
    "\n",
    "One thing that determinants tell us is the ratio of the area of this parallelogram to the area of the unit square. Use the `unit_square_mesh` to visualize this parallelogram directly for a scaling matrix. \n",
    "\n",
    "#### Q3 Using only this geometric information, what's the determinant of a scaling matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> **It's the scaling factor squared.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a very special \"scaling\" matrix above, there called `eye`.\n",
    "\n",
    "#### Q4 What's so special about `eye`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** It maps each vector to itself! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of viewing the entries of a matrix is that the columns tell you where your basis vectors go.\n",
    "\n",
    "#### Q5 Explain why `eye` is special from this point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** We can see that each basis vector is mapped to itself. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rank1` and `rank0`\n",
    "\n",
    "These matrices are called \"non-invertible\" -- there is no way to \"undo\" the transformation they perform.\n",
    "\n",
    "#### Q6 Based on the animation, can you explain why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** They squish multiple points to the same value -- that's not something you can undo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the determinant of a low-rank matrix (with `np.linalg.det`).\n",
    "\n",
    "#### Q7  What do you get? Give an explanation for this number in terms of the \"area of parallelograms\" interpretation of the determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** You get 0. The unit square gets mapped to a line or a point, and the area of a line or a point is 0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `rot`ations\n",
    "\n",
    "The function `utils.make_rotation` will make a matrix that rotates the input by an angle `theta`. This works in radians, not degrees, so if you want to make a 90 degree rotation, you should input $\\frac{\\pi}{2}$. In Python, $\\pi$ is called `np.pi`. Make a 180 degree rotation matrix, `rot180`. You'll use it in the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `reflection`s\n",
    "\n",
    "We call some transformations \"reflections\" because they act like mirrors -- they transform points on one side of some axis (aka line) to their mirror images on the other side. \n",
    "\n",
    "#### Q8 What are the axes around which the `negative_reflection` and `simple_reflection` reflect points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** `simple` reflects around the `y=x` axis, while `negative` reflects around the `y=-x` axis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rot180` and `negative_reflection` matrices both send the `unit_square_mesh` to the bottom-left quadrant. These two transformations aren't the same though. \n",
    "\n",
    "#### Q9 How are the transformations `rot180` and `negative_reflection` different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** The `reflection` changes the orientation of the basis vectors: basis vector 1 ends up clockwise of basis vector 2, when it began counter-clockwise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Matrices\n",
    "\n",
    "We can combine two transformations into one using matrix multiplication, implemented in Python with `np.dot`, the same function that lets us compute the dot products of vectors. The mathematical term for combining two things this way is \"composition\". Combine a scaling and rotation matrix together and observe the output.\n",
    "\n",
    "Fun fact: you can think of multiplying *complex numbers*, or numbers that can have both real and imaginary parts, as multiplying a vector whose components are the real and imaginary parts of one number by a 2x2 matrix that is a composition of rotation and scaling. The rotation angle $\\theta$ and scaling value $r$ for creating this matrix come from the polar representation of the complex number: $r\\mathrm{e}^{i\\theta}$. This is all you need to show that $e^{i\\pi} +1 = 0$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors and Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All matrices have special vectors on which they act like a scaling plus rotation matrix -- they rotate these vectors by some amount and then scale them by some value. Because linear algebra ideas were originally formalized in German, these vectors have a German name: *eigenvectors*. *Eigen-* means \"own\" or \"personal\", as in \"a man after my own heart\". Because these vectors are so closely associated with the matrix, they are its \"personal\" vectors.\n",
    "\n",
    "In addition to thinking about these special vectors, we also want to think about how these vectors are rotated and scaled -- if we know the special vectors and we know how they're rotated and scaled, then we can figure out whatever we need to know about this transformation.\n",
    "\n",
    "In statistics, we mostly care about transformations that don't rotate their eigenvectors (more technically: their eigenvalues are *real* rather than *imaginary* or *complex*). Change the argument `plot_eigenvectors` in `setup_plot` above to `True` and then look at all of the transformations above. When this argument is `True`, the eigenvectors of the transformation will be plotted, so long as they are the kind that only get scaled.\n",
    "\n",
    "#### Q10 Which transformations have these kinds of eigenvectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** The lower-rank matrices, the scalings, the shear, and the reflections. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the eigenvalues and eigenvectors using `np.linalg.eig`.\n",
    "\n",
    "Check the eigenvalues of all of the transformations you listed in repsonse to the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transform in [scaling1,scaling2,shear,rank0,rank1,simple_reflection,negative_reflection]:\n",
    "    print(transform)\n",
    "    eigenvalues,eigenvectors = np.linalg.eig(transform) #your code here\n",
    "    print(eigenvalues)\n",
    "    print(eigenvectors)\n",
    "    print('=*='*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, a complex or imaginary number looks like `a+bj`, as in: `0.057+1.345j`.\n",
    "\n",
    "#### Q11 Are the eigenvalues all, in fact, real numbers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** They sure are! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the eigenvalues of the `rank1` and `rank0` matrices.\n",
    "\n",
    "#### Q12 Do you notice anything special about them? Can you explain it? Hint: the determinant is the product of the eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"1874CD\"> ** They have at least 1 zero eigenvalue. This means that the determinant is 0, which makes sense because these transformations map the unit square to a line segment or a point, which has area 0.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
