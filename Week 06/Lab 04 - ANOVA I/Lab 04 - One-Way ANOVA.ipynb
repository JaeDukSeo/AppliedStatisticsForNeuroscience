{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/HWNI_logo.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04 - One-Way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# makes our plots show up inside Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "# choose colors that work for most color-blind folks\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import util.lab04utils as utils \n",
    "\n",
    "# this makes our tables easier to read\n",
    "utils.formatDataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this week's lab, we'll be using some EEG data graciously provided by the [Voytek lab](http://voyteklab.com/about-us/) of UCSD. Participants of varying ages were asked to perform a working memory task with varying levels of difficulty. The raw EEG signal has been summarized into the following two measures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Contralateral Delay Activity](https://www.ncbi.nlm.nih.gov/pubmed/26802451), or CDA, is used to measure the engagement of visual working memory.\n",
    "\n",
    "* [Frontal Midline Theta](https://www.ncbi.nlm.nih.gov/pubmed/9895201) oscillation amplitude has been correlated with sustained, internally-directed cognitive activity.\n",
    "\n",
    "The performance of the subjects has also been summarized using the measure\n",
    "[d'](https://en.wikipedia.org/wiki/Sensitivity_index) (pronounced \"d-prime\"), also known as the *sensitivity index*. D' is a measure of the subject's performance in  a task. It's based on comparing the true positive rate and false positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data and take a look at a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/voytek_working_memory_aging_split.csv',index_col=None)\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this lab, we're interested only in how task difficulty affects our three measures. We're uninterested in the subject's metadata -- `age_split`, `group`, `age`, and `idx`. Let's begin by dropping those columns from our dataframe using the DataFrame method `drop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = df.drop(['age_split','group','age','idx'], axis=1)\n",
    "data[data.id == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to keep an original copy of your dataframe around (here, named `df`) so you can undo irreversible changes, like dropping columns.\n",
    "\n",
    "If we're interested in looking at subject-by-subject information, we're not quite done with formatting our data. For subject-level analysis, our data is not yet tidy, since a single subject's observations are scattered over multiple rows. Use the `pivot` method to tidy our data. Hint: we want to get our row `index`es from the subject's `id` and to make new `columns` for our measures using the level of `difficulty`. Look back at the last lab for an example of using `pivot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivotData = data.pivot(index=,columns=) # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivotData.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It now takes two pieces of information to pick out a particular column: the measure we're interested in (one of `d`, `cda`, or `fmt`) and the `difficulty` level: `1`, `2`, or `3`. These need to be provided in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measure = 'cda'\n",
    "difficulty = 2\n",
    "\n",
    "pivotData[measure,difficulty].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we instead only index into the first level, we get a single-level dataframe back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivotData[measure].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll be running ANOVA using difficulty as an independent factor and the three measures as dependent factors. Choose a visualization or collection of visualizations that you think would be appropriate for this purpose and plot the data below. Be ready to explain your choice in class.\n",
    "\n",
    "You can use either the `pivot`ed data frame or the un`pivot`ed data frame.\n",
    "\n",
    "Hint: the easiest way to apply most of our visualizations to this dataset is to write a loop that runs over our measures and produces a separate plot for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the assumptions behind ANOVA.\n",
    "\n",
    "#### Q1 Based off of your visualization, do you think any of the assumptions of ANOVA are being violated for the case of fmt? What about d'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA the Easy Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll use the built-in `scipy.stats` function `f_oneway` to perform ANOVA. This will be useful for checking your work in the next problem where you write your own \"homebrew\" ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scipy.stats.f_oneway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to organize the code you write in the cell below so that you minimize the amount you repeat yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA the Hard Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better understanding of ANOVA, we'll now implement it from scratch.\n",
    "\n",
    "To get started, you'll need the total number of observations $N$, the group size (here, each group is the same size), and the keys for each group (here, 1, 2, and 3, and they're stored in the second level of the column multi-index).\n",
    "\n",
    "The first cell picks a measure to run ANOVA on. We'll want to write all of our code that follows in such a way that we can run ANOVA on the other measures just by changing this one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measure = '' # you choose: fmt, cda, or d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = len(df[measure])\n",
    "groupSize = len(pivotData[measure])\n",
    "groups = pivotData.columns.levels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll proceed by generating a new data frame that contains all the information we need to perform an ANOVA -- each row will contain the grand mean and the group mean, the explained component, and the residual for that observation.\n",
    "\n",
    "Because we'll be doing a between-subjects (aka unblocked) ANOVA, we use the un`pivot`ed data.\n",
    "\n",
    "#### Q2 Why is this a sub-optimal design for our statistical analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anovaFrame = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, compute the grand mean and group means and store them in the ANOVA dataframe. Hint: the `groupyby` method is useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anovaFrame[\"grandMean\"] = [] # your code here\n",
    "\n",
    "groupMeans = [] # your code here\n",
    "\n",
    "for group in groups:\n",
    "    anovaFrame.loc[anovaFrame.difficulty==group,\"groupMean\"] = [] # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anovaFrame.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only three unique values in the `groupMean` column, corresponding to the three group means. Calculate their average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupMeans = anovaFrame[\"groupMean\"].unique()\n",
    "\n",
    "print(groupMeans)\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 What other quantity is this equal to? What does this have to do with degrees of freedom?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the explained and unexplained components for each observation. The explained difference is the differences between the group average and the overall average. The unexplained difference is the difference between the individual score and the group average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anovaFrame[\"explained\"] = [] # your code here\n",
    "\n",
    "anovaFrame[\"residual\"] = [] # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, confirm that the total value for each observation is equal to the sum of the grand mean, the explained component, and the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert np.isclose(anovaFrame[measure],anovaFrame[\"grandMean\"] \n",
    "                                        + anovaFrame[\"explained\"]\n",
    "                                          + anovaFrame[\"residual\"]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a sum-of-squares function using `np.sum` and `np.square` and then use it to compute the following sum of squares values:\n",
    "\n",
    "- total sum of squares\n",
    "- sum of the grand mean squared\n",
    "- sum of squares explained by the model\n",
    "- residual sum of squares (component not explained by the model)\n",
    "\n",
    "Also, calculate the explainable sum of squares from the difference of two of the above quantities.\n",
    "\n",
    "The assertion statements in the final code block can be used to check your work.\n",
    "\n",
    "We'll store the sums of squares in a dictionary, `sumofsquares`, using the column name as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SS(x):\n",
    "    return # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumofsquares = {}\n",
    "\n",
    "keys = [measure,\"grandMean\",\"explained\",\"residual\"]\n",
    "\n",
    "for key in keys:\n",
    "    sumofsquares[key] = [] #your code here\n",
    "    \n",
    "sumofsquares[\"explainable\"] = [] # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these should be the same, except for computer rounding error\n",
    "\n",
    "assert( sumofsquares[measure] - (sumofsquares[\"grandMean\"] + \n",
    "                                 sumofsquares[\"explainable\"]) <= 1e-4 )\n",
    "\n",
    "assert( sumofsquares[\"explainable\"] - (sumofsquares[\"explained\"] +\n",
    "                                       sumofsquares[\"residual\"]) <= 1e-4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate the following degrees of freedom in this model:\n",
    "\n",
    "- total degrees of freedom\n",
    "- the degrees of freedom of the model (or explained degrees of freedom)\n",
    "- the \"leftover\" degrees of freedom (or the unexplained degrees of freedom)\n",
    "\n",
    "#### Q4 The latter two should add up to $N-1$. Where does the other degree of freedom \"go\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k is the number of groups\n",
    "k = len(groups)\n",
    "\n",
    "dof = {}\n",
    "vals = [] # your code here\n",
    "\n",
    "for key,val in zip(keys,vals):\n",
    "    dof[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(sum([dof[key] for key in dof.keys()]) == 2*N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate our estimate for the mean square of the explained and unexplained components. Note that, because we are estimating a parameter of the population, we want to use the appropriate degree of freedom instead of the raw $N$ for each average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meansquare = {}\n",
    "\n",
    "for key in [\"explained\",\"residual\"]:\n",
    "    meansquare[key] = [] # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean square of the explained component tells us how much, on average, our hypothesis is able to improve, in terms of squared error, our guess of the value of our outcome variable over the \"null\" hypothesis. The bigger this is, the more supported our hypothesis is, and the less likely we are to have observed such a result if the null hypothesis were true.\n",
    "\n",
    "However, a mean square value by itself doesn't tell you much  -- is reduction of 2 in mean squared error a \"big\" improvement? For our data, it would be, but for data with units in the billions and spread in the millions, it would not be. Therefore, if we want a statistic that tells us how good our hypothesis is, we need to somehow take into account the amount of unexplained variance.\n",
    "\n",
    "The statistic used for this purpose in ANOVA is the *F-statistic*, named in honor of its inventor, [Sir Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher). Compute the value of F for this data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F = [] # your code here\n",
    "\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret this F value and so determine if the amount of variance we were able to explain is close to what one would expect by chance, we need to get a hold of the sampling distribution of the F-statistic. Once we have that, we can calculate the area under the curve from the observed value on up and use that to determine our p-value.\n",
    "\n",
    "#### Q5 Why do we only calculate a \"one-tailed\" area, above our value, rather than a \"two-tailed\" area? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options for acquiring the sampling distribution of F:\n",
    "\n",
    "1. Use a pre-calculated distribution. In the old days, that'd mean looking up values in a table in a reference text. These days, statistical computing libraries like `scipy` provide this service via functions.\n",
    "1. Simulate what our data would look like under the null hypothesis and use the distribution of the F-statistic from our simulations to approximate the true distribution of F\n",
    "\n",
    "We'll take the second road.\n",
    "This will involve a [*resampling*](https://en.wikipedia.org/wiki/Resampling_%28statistics%29) technique called an\n",
    "[*approximate permutation test*](https://en.wikipedia.org/wiki/Resampling_%28statistics%29#Monte_Carlo_testing).\n",
    "It is closely related to [exact tests](https://en.wikipedia.org/wiki/Exact_test),\n",
    "which are covered in Chapter 7 of *Intuitive Biostatistics*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 Explain, in your own words, what the null hypothesis of the ANOVA test is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7 Under the null hypothesis of ANOVA, what can we say about the relationship between the group label and the measurement? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8 Based on this relationship, how could we simulate what our data would look like under the null hypothesis? That is, how could we generate data that is distributed according to the null hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `utils.estimate_f_distribution` will implement this simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measure = \"cda\"\n",
    "\n",
    "groupedData = utils.groupData(pivotData,measure,groups)\n",
    "\n",
    "fs = utils.estimate_f_distribution(groupedData)\n",
    "\n",
    "sortedFs = sorted(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.plotApproximatedF(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `sortedFs` array, calculate an approximate p-value for the value of F you calculated above. Check your work by comparing it to the value provided by `scipy.stats.f_oneway`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the process of generating our estimated F distribution, we need to generate a bunch of data sets that are distributed according to the null hypothesis. We can use these, plus any method that calculates p-values, to simulate the distribution of p-values under the null hypothesis. The function `utils.simulateNull` will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = utils.simulateNull(groupedData,N=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9 What sampling distribution do we expect the p-value to have under the null hypothesis? How might this distribution look different under the alternative hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, plot the simulated sampling distribution of the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond p - $\\eta^2$ and $\\omega^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F-statistic is used to determine the statistical significance of an ANOVA result.\n",
    "\n",
    "#### Q10 How is this different from the practical or scientific significance of an ANOVA result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APA recommends that, in addition to reporting F and p, scientists report the value $\\eta^2$, which is equal to the ratio of the explained sum-of-squares to the explainable sum-of-squares. It's also known as the *variance explained*.\n",
    "\n",
    "#### Q11 Why is this number closer to a notion of practical significance than F is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $\\eta$ for a statistically significant test you ran above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eta = [] # your code here\n",
    "eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12 Does this seem like a practically significant fraction to you? Look back at the visualizations of the data you produced at the beginning of the lab. Are there any visual hints that would lead you to expect a value of $\\eta^2$ close to what you calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\eta$ uses the sums-of-squares, rather than mean squares. This makes it a biased estimator of the quantity it is trying to capture: it overestimates how much variance has been explained. Intuitively, it only captures how well you explained your dataset, not how well you might explain additional data from the same population.\n",
    "\n",
    "An unbiased estimator for explained variance exists, known as $\\omega^2$ (pronounced \"omega-squared\"). You can [read more about it here](http://daniellakens.blogspot.com/2015/06/why-you-should-use-omega-squared.html).\n",
    "\n",
    "Use the following formula to compute $\\omega^2$ for your test:\n",
    "\n",
    "$$\n",
    "    \\omega^2 = \\frac{F-1}{\\frac{F+1+\\text{df}_{residual}}{\\text{df}_{explained}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "om_sqrd = [] # your code here\n",
    "om_sqrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13 Does this more accurate estimate change your opinion of the the practical significance of the results of your test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:neur299]",
   "language": "python",
   "name": "conda-env-neur299-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
